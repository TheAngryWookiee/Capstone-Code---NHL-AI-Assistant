{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eric\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.6250 - loss: 0.6642 - val_accuracy: 0.0330 - val_loss: 1.0623\n",
      "Epoch 2/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6298 - loss: 0.6494 - val_accuracy: 0.0283 - val_loss: 0.9732\n",
      "Epoch 3/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6249 - loss: 0.6508 - val_accuracy: 0.0529 - val_loss: 0.9804\n",
      "Epoch 4/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6341 - loss: 0.6438 - val_accuracy: 0.0364 - val_loss: 0.9537\n",
      "Epoch 5/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6286 - loss: 0.6450 - val_accuracy: 0.0961 - val_loss: 0.9517\n",
      "Epoch 6/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6314 - loss: 0.6440 - val_accuracy: 0.1216 - val_loss: 0.9400\n",
      "Epoch 7/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6334 - loss: 0.6388 - val_accuracy: 0.1247 - val_loss: 0.9770\n",
      "Epoch 8/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6388 - loss: 0.6345 - val_accuracy: 0.0746 - val_loss: 0.9950\n",
      "Epoch 9/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6335 - loss: 0.6328 - val_accuracy: 0.1127 - val_loss: 1.0085\n",
      "Epoch 10/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6405 - loss: 0.6297 - val_accuracy: 0.1743 - val_loss: 0.9289\n",
      "Epoch 11/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6422 - loss: 0.6278 - val_accuracy: 0.2345 - val_loss: 0.9758\n",
      "Epoch 12/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6489 - loss: 0.6229 - val_accuracy: 0.2078 - val_loss: 0.9287\n",
      "Epoch 13/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6440 - loss: 0.6233 - val_accuracy: 0.2094 - val_loss: 0.9212\n",
      "Epoch 14/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6502 - loss: 0.6162 - val_accuracy: 0.2676 - val_loss: 0.9385\n",
      "Epoch 15/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6549 - loss: 0.6144 - val_accuracy: 0.2753 - val_loss: 0.9249\n",
      "Epoch 16/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6586 - loss: 0.6076 - val_accuracy: 0.3110 - val_loss: 0.9118\n",
      "Epoch 17/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6579 - loss: 0.6069 - val_accuracy: 0.2867 - val_loss: 0.9418\n",
      "Epoch 18/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6592 - loss: 0.6024 - val_accuracy: 0.2881 - val_loss: 0.9172\n",
      "Epoch 19/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6686 - loss: 0.5939 - val_accuracy: 0.3780 - val_loss: 0.8563\n",
      "Epoch 20/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6685 - loss: 0.5912 - val_accuracy: 0.2951 - val_loss: 0.9275\n",
      "[LightGBM] [Info] Number of positive: 14411, number of negative: 14411\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000989 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3566\n",
      "[LightGBM] [Info] Number of data points in the train set: 28822, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Weakest areas for TOR: ['onIce_xGoalsPercentage', 'I_F_highDangerShots', 'onIce_corsiPercentage', 'offIce_xGoalsPercentage', 'penalties']\n",
      "Recommended trade targets:\n",
      "       season                name team position situation  games_played  \\\n",
      "74677    2024        Givani Smith  COL        R      5on5            13   \n",
      "73742    2024  Valtteri Puustinen  PIT        R      5on5            10   \n",
      "75167    2024    Tanner Laczynski  VGK        C      5on5             8   \n",
      "\n",
      "       I_F_missedShots  I_F_blockedShotAttempts  icetime  shifts  ...  \\\n",
      "74677                2                        3     4901     119  ...   \n",
      "73742                1                        1     5201     118  ...   \n",
      "75167                2                        2     4013      99  ...   \n",
      "\n",
      "       I_F_dZoneGiveaways  I_F_dZoneShiftStarts  I_F_neutralZoneShiftStarts  \\\n",
      "74677                   1                     9                          20   \n",
      "73742                   1                    10                          16   \n",
      "75167                   0                     4                          14   \n",
      "\n",
      "       faceoffsWon  faceoffsLost  penaltiesDrawn  shotsBlockedByPlayer  \\\n",
      "74677            0             0               3                     5   \n",
      "73742            0             0               1                     5   \n",
      "75167           16            20               1                     3   \n",
      "\n",
      "       total_shots  faceoffPercentage  predicted_fit  \n",
      "74677           10                NaN       0.363040  \n",
      "73742            6                NaN       0.335375  \n",
      "75167            6                0.8       0.290732  \n",
      "\n",
      "[3 rows x 47 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eric\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Eric\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('./NHL Datasets/All Skaters 08-25.csv')\n",
    "df_cup_data = pd.read_csv('./NHL Datasets/Stanley_Cup_Winners.csv')\n",
    "\n",
    "team_mapping = { \n",
    "    \"T.B\": \"TBL\",\n",
    "    \"N.J\": \"NJD\",\n",
    "    \"L.A\": \"LAK\",\n",
    "    \"S.J\": \"SJS\" \n",
    "}\n",
    "\n",
    "# Standardize team names in both datasets\n",
    "df['team'] = df['team'].replace(team_mapping)\n",
    "df_cup_data['winning_team'] = df_cup_data['winning_team'].replace(team_mapping)\n",
    "\n",
    "# Columns of interest, including 'season' for splitting purposes\n",
    "columns_of_interest = [\n",
    "    'season', 'name', 'team', 'position', 'situation', 'games_played', 'I_F_missedShots', 'I_F_blockedShotAttempts',\n",
    "    'icetime', 'shifts', 'gameScore', 'onIce_xGoalsPercentage', 'offIce_xGoalsPercentage', \n",
    "    'onIce_corsiPercentage', 'I_F_xOnGoal', 'I_F_xGoals', 'I_F_primaryAssists', \n",
    "    'I_F_secondaryAssists', 'I_F_shotsOnGoal', 'I_F_shotAttempts', 'I_F_points', 'I_F_goals', 'I_F_savedShotsOnGoal', \n",
    "    'penalties', 'I_F_faceOffsWon', 'I_F_hits', 'I_F_takeaways', 'I_F_giveaways', 'I_F_lowDangerShots', \n",
    "    'I_F_mediumDangerShots', 'I_F_highDangerShots', 'I_F_lowDangerxGoals', 'I_F_mediumDangerxGoals', 'I_F_highDangerxGoals',\n",
    "    'I_F_lowDangerGoals', 'I_F_mediumDangerGoals', 'I_F_highDangerGoals', 'I_F_dZoneGiveaways', \n",
    "    'I_F_dZoneShiftStarts', 'I_F_neutralZoneShiftStarts', 'faceoffsWon', 'faceoffsLost', 'penaltiesDrawn', 'shotsBlockedByPlayer'\n",
    "]\n",
    "\n",
    "# Exclude unwanted columns for evaluation\n",
    "columns_to_exclude = ['games_played']  \n",
    "analysis_columns = [col for col in columns_of_interest if col not in columns_to_exclude]\n",
    "\n",
    "df = df[columns_of_interest]\n",
    "\n",
    "# Filter for 5v5 situation\n",
    "df = df[df['situation'] == '5on5']\n",
    "\n",
    "# Handle missing values\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Feature engineering\n",
    "df['total_shots'] = df['I_F_shotsOnGoal'] + df['I_F_missedShots'] + df['I_F_blockedShotAttempts']\n",
    "\n",
    "# List of stats to normalize per game (removing 'I_F_oZoneShiftStarts')\n",
    "stats_per_game = [\n",
    "    'I_F_highDangerShots', 'I_F_savedShotsOnGoal', \n",
    "    'gameScore', 'onIce_xGoalsPercentage', 'offIce_xGoalsPercentage',\n",
    "    'onIce_corsiPercentage', 'I_F_xOnGoal', 'I_F_xGoals', 'I_F_primaryAssists', \n",
    "    'I_F_secondaryAssists', 'I_F_shotsOnGoal', 'I_F_shotAttempts', 'I_F_points', \n",
    "    'I_F_goals', 'I_F_savedShotsOnGoal', 'penalties', 'I_F_faceOffsWon', 'I_F_hits', \n",
    "    'I_F_takeaways', 'I_F_giveaways'\n",
    "]\n",
    "\n",
    "# Normalize stats per game\n",
    "for stat in stats_per_game:\n",
    "    df[stat] = df[stat] / df['games_played']\n",
    "\n",
    "df['faceoffPercentage'] = df['faceoffsWon'] / df['faceoffsLost']\n",
    "\n",
    "# Split data into train (<=2023) and test (2024)\n",
    "df_train = df[df['season'] <= 2023]\n",
    "df_test = df[df['season'] == 2024]\n",
    "\n",
    "# Define features and target variable\n",
    "features = [\n",
    "    'gameScore', 'onIce_xGoalsPercentage', 'I_F_xGoals', 'I_F_primaryAssists', 'I_F_secondaryAssists',\n",
    "    'I_F_shotsOnGoal', 'I_F_shotAttempts', 'I_F_goals', 'I_F_points', 'I_F_faceOffsWon', 'I_F_hits',\n",
    "    'I_F_takeaways', 'I_F_giveaways', 'I_F_highDangerxGoals'\n",
    "]\n",
    "\n",
    "df_cup_winners = df_train[df_train['team'].isin(df_cup_data['winning_team'])].copy()  \n",
    "df_cup_winners['winner'] = 1\n",
    "df_train = df_train.copy()\n",
    "df_train['winner'] = 0\n",
    "\n",
    "# Oversample winners\n",
    "df_winners_upsampled = df_cup_winners.sample(len(df_train), replace=True)\n",
    "\n",
    "# Combine with regular teams\n",
    "df_balanced = pd.concat([df_train, df_winners_upsampled])\n",
    "\n",
    "X_train = df_balanced[features]\n",
    "y_train = df_balanced['winner']\n",
    "\n",
    "X_test = df_test[features]\n",
    "y_test = df_test.get('winner', pd.Series([0] * len(df_test)))\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42),\n",
    "    \"LightGBM\": LGBMClassifier(n_estimators=100, learning_rate=0.1, random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"TensorFlow\": None  \n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "grid_search = GridSearchCV(GradientBoostingClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "models[\"GradientBoosting\"] = grid_search.best_estimator_\n",
    "\n",
    "tf_model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "tf_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "tf_model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "\n",
    "models[\"TensorFlow\"] = tf_model\n",
    "\n",
    "for name, model in models.items():\n",
    "    if name != \"TensorFlow\":\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "# Identify weaknesses\n",
    "def identify_weakest_areas(team_df, league_df, top_n=10):\n",
    "    numeric_cols = team_df.select_dtypes(include=['number']).columns\n",
    "    relevant_cols = [col for col in numeric_cols if col in stats_per_game]\n",
    "\n",
    "    team_avg = team_df[relevant_cols].mean()\n",
    "    league_avg = league_df[relevant_cols].mean()\n",
    "\n",
    "    weaknesses = (league_avg - team_avg).nlargest(top_n).index.tolist()\n",
    "    return weaknesses\n",
    "\n",
    "# Find trade target\n",
    "def find_trade_target(df, team_name, weaknesses, models, percentile_threshold=50):\n",
    "    team_players = df[df['team'] == team_name]\n",
    "    df = df[(df['team'] != team_name) & (df['season'] == 2024)]\n",
    "\n",
    "    for weakness in weaknesses:\n",
    "        threshold_value = df[weakness].quantile(percentile_threshold / 100.0)\n",
    "        df = df[df[weakness] >= threshold_value]\n",
    "\n",
    "    if df.empty:\n",
    "        return None\n",
    "\n",
    "    df['predicted_fit'] = (models[\"GradientBoosting\"].predict_proba(df[features])[:, 1] + \n",
    "                           models[\"RandomForest\"].predict_proba(df[features])[:, 1] + \n",
    "                           models[\"LightGBM\"].predict_proba(df[features])[:, 1]) / 3\n",
    "    return df.nlargest(3, 'predicted_fit')\n",
    "\n",
    "selected_team = \"TOR\"\n",
    "weakest_stats = identify_weakest_areas(df[df['team'] == selected_team], df, top_n=5)\n",
    "trade_recommendation = find_trade_target(df, selected_team, weakest_stats, models, percentile_threshold=50)\n",
    "\n",
    "if trade_recommendation is not None:\n",
    "    print(f\"Weakest areas for {selected_team}: {weakest_stats}\")\n",
    "    print(\"Recommended trade targets:\")\n",
    "    print(trade_recommendation)\n",
    "else:\n",
    "    print(\"No suitable trade candidates found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      " * Restarting with watchdog (windowsapi)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eric\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Save the trained TensorFlow model\n",
    "tf_model.save('saved_model.keras')\n",
    "\n",
    "# Function to load and make predictions\n",
    "def load_and_predict(input_data):\n",
    "    model = tf.keras.models.load_model('saved_model')\n",
    "    prediction = model.predict(input_data)\n",
    "    return prediction\n",
    "\n",
    "# Create a minimal Flask web app\n",
    "from flask import Flask, request, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.json\n",
    "    min_games_played = data.get('min_games_played', 0)  # Default to 0 if not provided\n",
    "    selected_team = data.get('team', None)\n",
    "    \n",
    "    # Filter data based on input\n",
    "    filtered_df = df[(df['games_played'] >= min_games_played)]\n",
    "    if selected_team:\n",
    "        filtered_df = filtered_df[filtered_df['team'] == selected_team]\n",
    "    \n",
    "    # Standardize and prepare for prediction\n",
    "    X_input = scaler.transform(filtered_df[features])\n",
    "    predictions = load_and_predict(X_input)\n",
    "\n",
    "    return jsonify({'predictions': predictions.tolist()})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eric\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.6135 - loss: 0.6632 - val_accuracy: 0.0196 - val_loss: 0.9942\n",
      "Epoch 2/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6273 - loss: 0.6521 - val_accuracy: 0.0559 - val_loss: 0.9786\n",
      "Epoch 3/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6285 - loss: 0.6472 - val_accuracy: 0.0175 - val_loss: 0.9682\n",
      "Epoch 4/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6321 - loss: 0.6433 - val_accuracy: 0.0219 - val_loss: 0.9791\n",
      "Epoch 5/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6304 - loss: 0.6430 - val_accuracy: 0.0810 - val_loss: 0.9541\n",
      "Epoch 6/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6300 - loss: 0.6422 - val_accuracy: 0.0250 - val_loss: 0.9964\n",
      "Epoch 7/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6359 - loss: 0.6378 - val_accuracy: 0.0541 - val_loss: 1.0106\n",
      "Epoch 8/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6356 - loss: 0.6343 - val_accuracy: 0.0940 - val_loss: 0.9749\n",
      "Epoch 9/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6381 - loss: 0.6319 - val_accuracy: 0.1044 - val_loss: 0.9409\n",
      "Epoch 10/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6462 - loss: 0.6269 - val_accuracy: 0.1847 - val_loss: 0.9109\n",
      "Epoch 11/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6472 - loss: 0.6272 - val_accuracy: 0.0500 - val_loss: 1.0633\n",
      "Epoch 12/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6532 - loss: 0.6195 - val_accuracy: 0.2274 - val_loss: 0.9107\n",
      "Epoch 13/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6538 - loss: 0.6145 - val_accuracy: 0.1971 - val_loss: 0.9708\n",
      "Epoch 14/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6526 - loss: 0.6157 - val_accuracy: 0.1140 - val_loss: 1.0826\n",
      "Epoch 15/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6542 - loss: 0.6126 - val_accuracy: 0.1563 - val_loss: 1.0080\n",
      "Epoch 16/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6618 - loss: 0.6059 - val_accuracy: 0.3539 - val_loss: 0.8746\n",
      "Epoch 17/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6588 - loss: 0.6053 - val_accuracy: 0.2533 - val_loss: 0.9882\n",
      "Epoch 18/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6671 - loss: 0.5966 - val_accuracy: 0.2395 - val_loss: 1.0017\n",
      "Epoch 19/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6655 - loss: 0.5989 - val_accuracy: 0.3322 - val_loss: 0.8884\n",
      "Epoch 20/20\n",
      "\u001b[1m721/721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6698 - loss: 0.5947 - val_accuracy: 0.3055 - val_loss: 0.9300\n",
      "[LightGBM] [Info] Number of positive: 14411, number of negative: 14411\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001795 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3566\n",
      "[LightGBM] [Info] Number of data points in the train set: 28822, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Weakest areas for TOR: ['onIce_xGoalsPercentage', 'I_F_highDangerShots', 'onIce_corsiPercentage', 'offIce_xGoalsPercentage', 'penalties']\n",
      "Recommended trade targets:\n",
      "       season                name team position situation  games_played  \\\n",
      "73742    2024  Valtteri Puustinen  PIT        R      5on5            10   \n",
      "75167    2024    Tanner Laczynski  VGK        C      5on5             8   \n",
      "74677    2024        Givani Smith  COL        R      5on5            13   \n",
      "\n",
      "       I_F_missedShots  I_F_blockedShotAttempts  icetime  shifts  ...  \\\n",
      "73742                1                        1     5201     118  ...   \n",
      "75167                2                        2     4013      99  ...   \n",
      "74677                2                        3     4901     119  ...   \n",
      "\n",
      "       I_F_dZoneGiveaways  I_F_dZoneShiftStarts  I_F_neutralZoneShiftStarts  \\\n",
      "73742                   1                    10                          16   \n",
      "75167                   0                     4                          14   \n",
      "74677                   1                     9                          20   \n",
      "\n",
      "       faceoffsWon  faceoffsLost  penaltiesDrawn  shotsBlockedByPlayer  \\\n",
      "73742            0             0               1                     5   \n",
      "75167           16            20               1                     3   \n",
      "74677            0             0               3                     5   \n",
      "\n",
      "       total_shots  faceoffPercentage  predicted_fit  \n",
      "73742            6                NaN       0.356928  \n",
      "75167            6                0.8       0.333189  \n",
      "74677           10                NaN       0.306492  \n",
      "\n",
      "[3 rows x 47 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eric\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Eric\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import joblib  # For saving and loading the scaler\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('./NHL Datasets/All Skaters 08-25.csv')\n",
    "df_cup_data = pd.read_csv('./NHL Datasets/Stanley_Cup_Winners.csv')\n",
    "\n",
    "team_mapping = { \n",
    "    \"T.B\": \"TBL\",\n",
    "    \"N.J\": \"NJD\",\n",
    "    \"L.A\": \"LAK\",\n",
    "    \"S.J\": \"SJS\" \n",
    "}\n",
    "\n",
    "# Standardize team names in both datasets\n",
    "df['team'] = df['team'].replace(team_mapping)\n",
    "df_cup_data['winning_team'] = df_cup_data['winning_team'].replace(team_mapping)\n",
    "\n",
    "# Columns of interest, including 'season' for splitting purposes\n",
    "columns_of_interest = [\n",
    "    'season', 'name', 'team', 'position', 'situation', 'games_played', 'I_F_missedShots', 'I_F_blockedShotAttempts',\n",
    "    'icetime', 'shifts', 'gameScore', 'onIce_xGoalsPercentage', 'offIce_xGoalsPercentage', \n",
    "    'onIce_corsiPercentage', 'I_F_xOnGoal', 'I_F_xGoals', 'I_F_primaryAssists', \n",
    "    'I_F_secondaryAssists', 'I_F_shotsOnGoal', 'I_F_shotAttempts', 'I_F_points', 'I_F_goals', 'I_F_savedShotsOnGoal', \n",
    "    'penalties', 'I_F_faceOffsWon', 'I_F_hits', 'I_F_takeaways', 'I_F_giveaways', 'I_F_lowDangerShots', \n",
    "    'I_F_mediumDangerShots', 'I_F_highDangerShots', 'I_F_lowDangerxGoals', 'I_F_mediumDangerxGoals', 'I_F_highDangerxGoals',\n",
    "    'I_F_lowDangerGoals', 'I_F_mediumDangerGoals', 'I_F_highDangerGoals', 'I_F_dZoneGiveaways', \n",
    "    'I_F_dZoneShiftStarts', 'I_F_neutralZoneShiftStarts', 'faceoffsWon', 'faceoffsLost', 'penaltiesDrawn', 'shotsBlockedByPlayer'\n",
    "]\n",
    "\n",
    "# Exclude unwanted columns for evaluation\n",
    "columns_to_exclude = ['games_played']  \n",
    "analysis_columns = [col for col in columns_of_interest if col not in columns_to_exclude]\n",
    "\n",
    "df = df[columns_of_interest]\n",
    "\n",
    "# Filter for 5v5 situation\n",
    "df = df[df['situation'] == '5on5']\n",
    "\n",
    "# Handle missing values\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Feature engineering\n",
    "df['total_shots'] = df['I_F_shotsOnGoal'] + df['I_F_missedShots'] + df['I_F_blockedShotAttempts']\n",
    "\n",
    "# List of stats to normalize per game (removing 'I_F_oZoneShiftStarts')\n",
    "stats_per_game = [\n",
    "    'I_F_highDangerShots', 'I_F_savedShotsOnGoal', \n",
    "    'gameScore', 'onIce_xGoalsPercentage', 'offIce_xGoalsPercentage',\n",
    "    'onIce_corsiPercentage', 'I_F_xOnGoal', 'I_F_xGoals', 'I_F_primaryAssists', \n",
    "    'I_F_secondaryAssists', 'I_F_shotsOnGoal', 'I_F_shotAttempts', 'I_F_points', \n",
    "    'I_F_goals', 'I_F_savedShotsOnGoal', 'penalties', 'I_F_faceOffsWon', 'I_F_hits', \n",
    "    'I_F_takeaways', 'I_F_giveaways'\n",
    "]\n",
    "\n",
    "# Normalize stats per game\n",
    "for stat in stats_per_game:\n",
    "    df[stat] = df[stat] / df['games_played']\n",
    "\n",
    "df['faceoffPercentage'] = df['faceoffsWon'] / df['faceoffsLost']\n",
    "\n",
    "# Split data into train (<=2023) and test (2024)\n",
    "df_train = df[df['season'] <= 2023]\n",
    "df_test = df[df['season'] == 2024]\n",
    "\n",
    "# Define features and target variable\n",
    "features = [\n",
    "    'gameScore', 'onIce_xGoalsPercentage', 'I_F_xGoals', 'I_F_primaryAssists', 'I_F_secondaryAssists',\n",
    "    'I_F_shotsOnGoal', 'I_F_shotAttempts', 'I_F_goals', 'I_F_points', 'I_F_faceOffsWon', 'I_F_hits',\n",
    "    'I_F_takeaways', 'I_F_giveaways', 'I_F_highDangerxGoals'\n",
    "]\n",
    "\n",
    "df_cup_winners = df_train[df_train['team'].isin(df_cup_data['winning_team'])].copy()  \n",
    "df_cup_winners['winner'] = 1\n",
    "df_train = df_train.copy()\n",
    "df_train['winner'] = 0\n",
    "\n",
    "# Oversample winners\n",
    "df_winners_upsampled = df_cup_winners.sample(len(df_train), replace=True)\n",
    "\n",
    "# Combine with regular teams\n",
    "df_balanced = pd.concat([df_train, df_winners_upsampled])\n",
    "\n",
    "X_train = df_balanced[features]\n",
    "y_train = df_balanced['winner']\n",
    "\n",
    "X_test = df_test[features]\n",
    "y_test = df_test.get('winner', pd.Series([0] * len(df_test)))\n",
    "\n",
    "# Standardize features and save the scaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Save the scaler for future use\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42),\n",
    "    \"LightGBM\": LGBMClassifier(n_estimators=100, learning_rate=0.1, random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"TensorFlow\": None  \n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "grid_search = GridSearchCV(GradientBoostingClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "models[\"GradientBoosting\"] = grid_search.best_estimator_\n",
    "\n",
    "tf_model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "tf_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "tf_model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "\n",
    "models[\"TensorFlow\"] = tf_model\n",
    "\n",
    "for name, model in models.items():\n",
    "    if name != \"TensorFlow\":\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "# Save the TensorFlow model for later use\n",
    "tf_model.save('saved_model.keras')\n",
    "\n",
    "# Identify weaknesses\n",
    "def identify_weakest_areas(team_df, league_df, top_n=10):\n",
    "    numeric_cols = team_df.select_dtypes(include=['number']).columns\n",
    "    relevant_cols = [col for col in numeric_cols if col in stats_per_game]\n",
    "\n",
    "    team_avg = team_df[relevant_cols].mean()\n",
    "    league_avg = league_df[relevant_cols].mean()\n",
    "\n",
    "    weaknesses = (league_avg - team_avg).nlargest(top_n).index.tolist()\n",
    "    return weaknesses\n",
    "\n",
    "# Find trade target\n",
    "def find_trade_target(df, team_name, weaknesses, models, percentile_threshold=50):\n",
    "    team_players = df[df['team'] == team_name]\n",
    "    df = df[(df['team'] != team_name) & (df['season'] == 2024)]\n",
    "\n",
    "    for weakness in weaknesses:\n",
    "        threshold_value = df[weakness].quantile(percentile_threshold / 100.0)\n",
    "        df = df[df[weakness] >= threshold_value]\n",
    "\n",
    "    if df.empty:\n",
    "        return None\n",
    "\n",
    "    df['predicted_fit'] = (models[\"GradientBoosting\"].predict_proba(df[features])[:, 1] + \n",
    "                           models[\"RandomForest\"].predict_proba(df[features])[:, 1] + \n",
    "                           models[\"LightGBM\"].predict_proba(df[features])[:, 1]) / 3\n",
    "    return df.nlargest(3, 'predicted_fit')\n",
    "\n",
    "selected_team = \"TOR\"\n",
    "weakest_stats = identify_weakest_areas(df[df['team'] == selected_team], df, top_n=5)\n",
    "trade_recommendation = find_trade_target(df, selected_team, weakest_stats, models, percentile_threshold=50)\n",
    "\n",
    "if trade_recommendation is not None:\n",
    "    print(f\"Weakest areas for {selected_team}: {weakest_stats}\")\n",
    "    print(\"Recommended trade targets:\")\n",
    "    print(trade_recommendation)\n",
    "else:\n",
    "    print(\"No suitable trade candidates found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
